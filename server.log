DEBUG: moondream_station loaded from: /home/bcoster/.moondream-station/moondream-station/moondream_station/__init__.py
[SDXL] Backend loaded successfully from local backends directory
Baseline VRAM: 1096MB (system + X Windows)
2026-01-02 19:58:23,903 - MoondreamServer - INFO - Loading manifest from /home/bcoster/.moondream-station/moondream-station/local_manifest.json
2026-01-02 19:58:23,903 - MoondreamServer - WARNING - DEV MODE ENABLED: Disabling torch.compile for faster startup and lower memory.
2026-01-02 19:58:24,184 - MoondreamServer - INFO - Starting service with model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
DEBUG: load_backend moondream_backend from /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
2026-01-02 19:58:24,184 - moondream_backend - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 19:58:24,188 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 0.00 MB
INFO:     Started server process [2917688]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://127.0.0.1:2020 (Press CTRL+C to quit)
2026-01-02 19:58:25,291 - MoondreamServer - INFO - Service started successfully on port 2020
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 20:04:58,552 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2539MB, Used=5653MB, Baseline=1096MB, Model=4556MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4556MB, RAM: 756MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 20:05:01,943 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6526MB, Used=1666MB, Baseline=1096MB, Model=569MB
Tracked model load: moondream-2 - VRAM: 569MB, RAM: 758MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 20:05:05,088 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2512MB, Used=5680MB, Baseline=1096MB, Model=4584MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4584MB, RAM: 795MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 20:05:31,709 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6378MB, Used=1814MB, Baseline=1096MB, Model=718MB
Tracked model load: moondream-2 - VRAM: 718MB, RAM: 795MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 20:05:34,912 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2562MB, Used=5630MB, Baseline=1096MB, Model=4534MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4534MB, RAM: 820MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 20:05:40,777 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6609MB, Used=1583MB, Baseline=1096MB, Model=487MB
Tracked model load: moondream-2 - VRAM: 487MB, RAM: 820MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 20:05:43,872 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2585MB, Used=5607MB, Baseline=1096MB, Model=4511MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4511MB, RAM: 822MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 20:05:50,959 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4076.19 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=2565MB, Used=5627MB, Baseline=1096MB, Model=4531MB
Tracked model load: moondream-2 - VRAM: 4531MB, RAM: 822MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=6631MB, Used=1561MB, Baseline=1096MB, Model=464MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 464MB, RAM: 885MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 20:05:52,670 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6632MB, Used=1560MB, Baseline=1096MB, Model=464MB
Tracked model load: moondream-2 - VRAM: 464MB, RAM: 885MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 20:05:54,044 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 20:10:14,300 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2733MB, Used=5459MB, Baseline=1096MB, Model=4363MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4363MB, RAM: 950MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 20:10:16,043 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6808MB, Used=1384MB, Baseline=1096MB, Model=288MB
Tracked model load: moondream-2 - VRAM: 288MB, RAM: 950MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 20:10:19,229 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2789MB, Used=5403MB, Baseline=1096MB, Model=4307MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4307MB, RAM: 961MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 20:10:25,889 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6793MB, Used=1399MB, Baseline=1096MB, Model=303MB
Tracked model load: moondream-2 - VRAM: 303MB, RAM: 961MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 20:10:29,099 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2730MB, Used=5462MB, Baseline=1096MB, Model=4366MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4366MB, RAM: 961MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 20:10:35,941 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4076.19 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=2712MB, Used=5480MB, Baseline=1096MB, Model=4384MB
Tracked model load: moondream-2 - VRAM: 4384MB, RAM: 961MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=6776MB, Used=1416MB, Baseline=1096MB, Model=320MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 320MB, RAM: 1023MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 20:10:37,887 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 1081.00 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=5721MB, Used=2471MB, Baseline=1096MB, Model=1375MB
Tracked model load: moondream-2 - VRAM: 1375MB, RAM: 2087MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 20:10:39,183 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 20:20:47,673 - moondream_backend - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend
2026-01-02 20:20:48,053 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
[InferenceService] Unloading model...
[InferenceService] Unloading backends from manifest...
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 20:25:20,739 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6470MB, Used=1722MB, Baseline=1096MB, Model=626MB
Tracked model load: moondream-2 - VRAM: 626MB, RAM: 1017MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 20:25:23,891 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2465MB, Used=5727MB, Baseline=1096MB, Model=4630MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4630MB, RAM: 1017MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 20:25:26,899 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6507MB, Used=1685MB, Baseline=1096MB, Model=588MB
Tracked model load: moondream-2 - VRAM: 588MB, RAM: 1017MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 20:25:30,066 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2484MB, Used=5708MB, Baseline=1096MB, Model=4612MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4612MB, RAM: 1017MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 20:25:35,247 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6580MB, Used=1612MB, Baseline=1096MB, Model=516MB
Tracked model load: moondream-2 - VRAM: 516MB, RAM: 1017MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 20:25:38,347 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2482MB, Used=5710MB, Baseline=1096MB, Model=4614MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4614MB, RAM: 1028MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 20:25:44,196 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4076.19 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=2460MB, Used=5732MB, Baseline=1096MB, Model=4636MB
Tracked model load: moondream-2 - VRAM: 4636MB, RAM: 1028MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=6549MB, Used=1643MB, Baseline=1096MB, Model=547MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 547MB, RAM: 1028MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 20:25:45,821 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6535MB, Used=1657MB, Baseline=1096MB, Model=561MB
Tracked model load: moondream-2 - VRAM: 561MB, RAM: 1043MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 20:25:47,298 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 20:26:49,583 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
[InferenceService] Unloading model...
[InferenceService] Unloading backends from manifest...
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 20:40:48,968 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6362MB, Used=1830MB, Baseline=1096MB, Model=734MB
Tracked model load: moondream-2 - VRAM: 734MB, RAM: 1036MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 20:40:52,023 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2226MB, Used=5966MB, Baseline=1096MB, Model=4869MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4869MB, RAM: 1036MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 20:40:55,335 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6298MB, Used=1894MB, Baseline=1096MB, Model=798MB
Tracked model load: moondream-2 - VRAM: 798MB, RAM: 1036MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 20:40:58,440 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2300MB, Used=5892MB, Baseline=1096MB, Model=4796MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4796MB, RAM: 1036MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 20:41:02,965 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6342MB, Used=1850MB, Baseline=1096MB, Model=754MB
Tracked model load: moondream-2 - VRAM: 754MB, RAM: 1036MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 20:41:06,046 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4076.19 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2299MB, Used=5893MB, Baseline=1096MB, Model=4797MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4797MB, RAM: 1035MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 20:41:12,574 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4210.42 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=2193MB, Used=5999MB, Baseline=1096MB, Model=4903MB
Tracked model load: moondream-2 - VRAM: 4903MB, RAM: 1035MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=6352MB, Used=1840MB, Baseline=1096MB, Model=744MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 744MB, RAM: 1036MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 20:41:13,626 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6358MB, Used=1834MB, Baseline=1096MB, Model=738MB
Tracked model load: moondream-2 - VRAM: 738MB, RAM: 1036MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 20:41:15,596 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 20:55:51,405 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2654MB, Used=5538MB, Baseline=1096MB, Model=4442MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4442MB, RAM: 1043MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 20:55:52,766 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6736MB, Used=1456MB, Baseline=1096MB, Model=360MB
Tracked model load: moondream-2 - VRAM: 360MB, RAM: 1043MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 20:55:56,142 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 20:55:56,914 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 20:55:57,433 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6708MB, Used=1484MB, Baseline=1096MB, Model=388MB
Tracked model load: moondream-2 - VRAM: 388MB, RAM: 1043MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 20:56:00,500 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2686MB, Used=5506MB, Baseline=1096MB, Model=4410MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4410MB, RAM: 1043MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 20:56:03,507 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6714MB, Used=1478MB, Baseline=1096MB, Model=382MB
Tracked model load: moondream-2 - VRAM: 382MB, RAM: 1043MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 20:56:06,863 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 20:56:07,809 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 20:56:08,301 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6692MB, Used=1500MB, Baseline=1096MB, Model=404MB
Tracked model load: moondream-2 - VRAM: 404MB, RAM: 1043MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
2026-01-02 20:56:11,427 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2678MB, Used=5514MB, Baseline=1096MB, Model=4418MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4418MB, RAM: 1043MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 20:56:18,780 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4076.19 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=2659MB, Used=5533MB, Baseline=1096MB, Model=4437MB
Tracked model load: moondream-2 - VRAM: 4437MB, RAM: 1043MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=6715MB, Used=1477MB, Baseline=1096MB, Model=380MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 380MB, RAM: 1059MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 20:56:20,825 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 1633.26 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=5095MB, Used=3097MB, Baseline=1096MB, Model=2000MB
Tracked model load: moondream-2 - VRAM: 2000MB, RAM: 2673MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 20:56:22,307 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 20:56:23,690 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 20:56:24,312 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6688MB, Used=1504MB, Baseline=1096MB, Model=408MB
Tracked model load: moondream-2 - VRAM: 408MB, RAM: 1044MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 20:56:26,463 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 20:56:26,483 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 20:56:27,127 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6698MB, Used=1494MB, Baseline=1096MB, Model=398MB
Tracked model load: moondream-2 - VRAM: 398MB, RAM: 1044MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 20:56:30,064 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4076.19 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2645MB, Used=5547MB, Baseline=1096MB, Model=4450MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4450MB, RAM: 1044MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 20:56:36,689 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4210.42 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=2525MB, Used=5667MB, Baseline=1096MB, Model=4571MB
Tracked model load: moondream-2 - VRAM: 4571MB, RAM: 1044MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2665MB, Used=5527MB, Baseline=1096MB, Model=4430MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4430MB, RAM: 1044MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 20:56:38,102 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=2660MB, Used=5532MB, Baseline=1096MB, Model=4436MB
Tracked model load: moondream-2 - VRAM: 4436MB, RAM: 1059MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 20:56:42,104 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 4271204352
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 20:56:42,620 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
VRAM: Total=8192MB, Free=2655MB, Used=5537MB, Baseline=1096MB, Model=4441MB
Tracked model load: moondream-2 - VRAM: 4441MB, RAM: 1056MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 20:56:44,228 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 4313671680
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 20:56:44,776 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 5025.16 MB
VRAM: Total=8192MB, Free=1665MB, Used=6527MB, Baseline=1096MB, Model=5431MB
Tracked model load: moondream-2 - VRAM: 5431MB, RAM: 2065MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=6764MB, Used=1428MB, Baseline=1096MB, Model=332MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 332MB, RAM: 1105MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 20:57:05,251 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6752MB, Used=1440MB, Baseline=1096MB, Model=344MB
Tracked model load: moondream-2 - VRAM: 344MB, RAM: 1105MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 20:57:08,303 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 20:57:09,081 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:01:42,661 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6056MB, Used=2136MB, Baseline=1096MB, Model=1040MB
Tracked model load: moondream-2 - VRAM: 1040MB, RAM: 1047MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:01:45,715 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2093MB, Used=6099MB, Baseline=1096MB, Model=5003MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 5003MB, RAM: 1047MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:01:49,382 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6137MB, Used=2055MB, Baseline=1096MB, Model=959MB
Tracked model load: moondream-2 - VRAM: 959MB, RAM: 1047MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:01:52,456 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 21:01:53,308 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:01:53,848 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6160MB, Used=2032MB, Baseline=1096MB, Model=936MB
Tracked model load: moondream-2 - VRAM: 936MB, RAM: 1047MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:01:56,795 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4076.19 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2138MB, Used=6054MB, Baseline=1096MB, Model=4958MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4958MB, RAM: 1047MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:02:02,124 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4210.42 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=2018MB, Used=6174MB, Baseline=1096MB, Model=5078MB
Tracked model load: moondream-2 - VRAM: 5078MB, RAM: 1047MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=6201MB, Used=1991MB, Baseline=1096MB, Model=895MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 895MB, RAM: 1055MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:02:03,237 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6202MB, Used=1990MB, Baseline=1096MB, Model=894MB
Tracked model load: moondream-2 - VRAM: 894MB, RAM: 1055MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:02:05,231 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 21:02:06,112 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:02:06,623 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6182MB, Used=2010MB, Baseline=1096MB, Model=914MB
Tracked model load: moondream-2 - VRAM: 914MB, RAM: 1102MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weightsDEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']

Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:02:08,555 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 21:02:09,546 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:02:10,135 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6162MB, Used=2030MB, Baseline=1096MB, Model=934MB
Tracked model load: moondream-2 - VRAM: 934MB, RAM: 1102MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:02:12,151 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2441MB, Used=5751MB, Baseline=1096MB, Model=4655MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4655MB, RAM: 1110MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:02:21,106 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=2441MB, Used=5751MB, Baseline=1096MB, Model=4654MB
Tracked model load: moondream-2 - VRAM: 4654MB, RAM: 1110MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 21:02:26,915 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 4271204352
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:05:15,637 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6609MB, Used=1583MB, Baseline=1096MB, Model=487MB
Tracked model load: moondream-2 - VRAM: 487MB, RAM: 1112MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:05:18,966 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2605MB, Used=5587MB, Baseline=1096MB, Model=4491MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4491MB, RAM: 1112MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:05:21,787 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6647MB, Used=1545MB, Baseline=1096MB, Model=449MB
Tracked model load: moondream-2 - VRAM: 449MB, RAM: 1112MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:05:24,856 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 21:05:25,650 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:05:26,170 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6610MB, Used=1582MB, Baseline=1096MB, Model=486MB
Tracked model load: moondream-2 - VRAM: 486MB, RAM: 1112MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:05:29,548 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4076.19 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2577MB, Used=5615MB, Baseline=1096MB, Model=4519MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4519MB, RAM: 1112MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:05:34,484 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4210.42 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=2477MB, Used=5715MB, Baseline=1096MB, Model=4619MB
Tracked model load: moondream-2 - VRAM: 4619MB, RAM: 1112MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=6639MB, Used=1553MB, Baseline=1096MB, Model=456MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 456MB, RAM: 1112MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:05:35,711 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6639MB, Used=1553MB, Baseline=1096MB, Model=456MB
Tracked model load: moondream-2 - VRAM: 456MB, RAM: 1127MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:05:37,427 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 21:05:38,437 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:05:38,993 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6634MB, Used=1558MB, Baseline=1096MB, Model=461MB
Tracked model load: moondream-2 - VRAM: 461MB, RAM: 1112MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:05:41,094 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 21:05:42,048 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:05:42,656 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6654MB, Used=1538MB, Baseline=1096MB, Model=442MB
Tracked model load: moondream-2 - VRAM: 442MB, RAM: 1127MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:05:44,288 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2689MB, Used=5503MB, Baseline=1096MB, Model=4407MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4407MB, RAM: 1112MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:05:51,891 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=2689MB, Used=5503MB, Baseline=1096MB, Model=4406MB
Tracked model load: moondream-2 - VRAM: 4406MB, RAM: 1112MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 21:05:55,928 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 4271204352
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:06:11,227 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6699MB, Used=1493MB, Baseline=1096MB, Model=396MB
Tracked model load: moondream-2 - VRAM: 396MB, RAM: 1112MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:06:14,439 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2651MB, Used=5541MB, Baseline=1096MB, Model=4445MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4445MB, RAM: 1112MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:06:17,588 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6729MB, Used=1463MB, Baseline=1096MB, Model=367MB
Tracked model load: moondream-2 - VRAM: 367MB, RAM: 1112MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
2026-01-02 21:06:20,697 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 21:06:21,633 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:06:22,161 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6703MB, Used=1489MB, Baseline=1096MB, Model=393MB
Tracked model load: moondream-2 - VRAM: 393MB, RAM: 1112MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:06:24,624 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 18.02 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=6698MB, Used=1494MB, Baseline=1096MB, Model=398MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 398MB, RAM: 1139MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:06:26,562 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 2593.67 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=4179MB, Used=4013MB, Baseline=1096MB, Model=2917MB
Tracked model load: moondream-2 - VRAM: 2917MB, RAM: 3699MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:06:27,619 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=6730MB, Used=1462MB, Baseline=1096MB, Model=365MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 365MB, RAM: 1112MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:06:29,705 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6745MB, Used=1447MB, Baseline=1096MB, Model=350MB
Tracked model load: moondream-2 - VRAM: 350MB, RAM: 1112MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:06:31,649 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 21:06:32,840 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:06:33,357 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6782MB, Used=1410MB, Baseline=1096MB, Model=314MB
Tracked model load: moondream-2 - VRAM: 314MB, RAM: 1113MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:06:35,504 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 21:06:36,350 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:06:36,874 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6810MB, Used=1382MB, Baseline=1096MB, Model=286MB
Tracked model load: moondream-2 - VRAM: 286MB, RAM: 1128MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:06:38,391 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=6806MB, Used=1386MB, Baseline=1096MB, Model=290MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 290MB, RAM: 1113MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:06:40,101 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6806MB, Used=1386MB, Baseline=1096MB, Model=290MB
Tracked model load: moondream-2 - VRAM: 290MB, RAM: 1113MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:06:42,027 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=6806MB, Used=1386MB, Baseline=1096MB, Model=290MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 290MB, RAM: 1128MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:06:43,876 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6806MB, Used=1386MB, Baseline=1096MB, Model=290MB
Tracked model load: moondream-2 - VRAM: 290MB, RAM: 1128MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:06:45,309 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 21:06:46,442 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:06:46,951 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6808MB, Used=1384MB, Baseline=1096MB, Model=288MB
Tracked model load: moondream-2 - VRAM: 288MB, RAM: 1113MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:06:49,056 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 21:06:49,820 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:06:50,415 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6796MB, Used=1396MB, Baseline=1096MB, Model=299MB
Tracked model load: moondream-2 - VRAM: 299MB, RAM: 1115MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:06:52,268 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=6794MB, Used=1398MB, Baseline=1096MB, Model=302MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 302MB, RAM: 1113MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:06:54,127 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6794MB, Used=1398MB, Baseline=1096MB, Model=302MB
Tracked model load: moondream-2 - VRAM: 302MB, RAM: 1120MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:06:55,971 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=6798MB, Used=1394MB, Baseline=1096MB, Model=298MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 298MB, RAM: 1113MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:06:57,632 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6798MB, Used=1394MB, Baseline=1096MB, Model=298MB
Tracked model load: moondream-2 - VRAM: 298MB, RAM: 1113MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:06:59,532 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 21:07:00,572 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:07:01,102 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6801MB, Used=1391MB, Baseline=1096MB, Model=295MB
Tracked model load: moondream-2 - VRAM: 295MB, RAM: 1112MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:07:03,351 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 21:07:04,338 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 49374208
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:07:04,975 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 456.60 MB
VRAM: Total=8192MB, Free=6345MB, Used=1847MB, Baseline=1096MB, Model=751MB
Tracked model load: moondream-2 - VRAM: 751MB, RAM: 1572MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:07:06,321 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=6720MB, Used=1472MB, Baseline=1096MB, Model=375MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 375MB, RAM: 1113MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:07:08,195 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6723MB, Used=1469MB, Baseline=1096MB, Model=373MB
Tracked model load: moondream-2 - VRAM: 373MB, RAM: 1113MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:07:10,237 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=6738MB, Used=1454MB, Baseline=1096MB, Model=358MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 358MB, RAM: 1128MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:07:11,957 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6732MB, Used=1460MB, Baseline=1096MB, Model=364MB
Tracked model load: moondream-2 - VRAM: 364MB, RAM: 1134MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:07:13,344 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 21:07:14,500 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:07:15,153 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6616MB, Used=1576MB, Baseline=1096MB, Model=479MB
Tracked model load: moondream-2 - VRAM: 479MB, RAM: 1113MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:07:17,192 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 21:07:18,612 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:07:19,145 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6400MB, Used=1792MB, Baseline=1096MB, Model=696MB
Tracked model load: moondream-2 - VRAM: 696MB, RAM: 1113MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:07:21,036 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=6500MB, Used=1692MB, Baseline=1096MB, Model=596MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 596MB, RAM: 1113MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:07:22,911 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6387MB, Used=1805MB, Baseline=1096MB, Model=709MB
Tracked model load: moondream-2 - VRAM: 709MB, RAM: 1128MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:07:24,399 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 21:07:27,098 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 4271204352
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:07:27,620 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
VRAM: Total=8192MB, Free=2444MB, Used=5748MB, Baseline=1096MB, Model=4652MB
Tracked model load: moondream-2 - VRAM: 4652MB, RAM: 1113MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2467MB, Used=5725MB, Baseline=1096MB, Model=4629MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4629MB, RAM: 1149MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:07:29,806 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=2474MB, Used=5718MB, Baseline=1096MB, Model=4622MB
Tracked model load: moondream-2 - VRAM: 4622MB, RAM: 1113MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 21:07:36,027 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 4271204352
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:07:36,491 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
VRAM: Total=8192MB, Free=2470MB, Used=5722MB, Baseline=1096MB, Model=4625MB
Tracked model load: moondream-2 - VRAM: 4625MB, RAM: 1135MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:20:55,691 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2705MB, Used=5487MB, Baseline=1096MB, Model=4391MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4391MB, RAM: 1136MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:21:00,049 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6790MB, Used=1402MB, Baseline=1096MB, Model=305MB
Tracked model load: moondream-2 - VRAM: 305MB, RAM: 1114MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:21:03,537 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 21:21:04,416 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:21:04,905 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6714MB, Used=1478MB, Baseline=1096MB, Model=382MB
Tracked model load: moondream-2 - VRAM: 382MB, RAM: 1114MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:21:08,113 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2686MB, Used=5506MB, Baseline=1096MB, Model=4410MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4410MB, RAM: 1114MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:21:11,200 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6727MB, Used=1465MB, Baseline=1096MB, Model=369MB
Tracked model load: moondream-2 - VRAM: 369MB, RAM: 1114MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:21:14,557 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 21:21:15,523 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:21:16,098 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6716MB, Used=1476MB, Baseline=1096MB, Model=380MB
Tracked model load: moondream-2 - VRAM: 380MB, RAM: 1114MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:21:19,021 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4076.19 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2666MB, Used=5526MB, Baseline=1096MB, Model=4430MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4430MB, RAM: 1114MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:21:25,889 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4210.44 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=2554MB, Used=5638MB, Baseline=1096MB, Model=4542MB
Tracked model load: moondream-2 - VRAM: 4542MB, RAM: 1114MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=6758MB, Used=1434MB, Baseline=1096MB, Model=338MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 338MB, RAM: 1114MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:21:27,029 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6753MB, Used=1439MB, Baseline=1096MB, Model=342MB
Tracked model load: moondream-2 - VRAM: 342MB, RAM: 1114MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:21:29,089 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 21:21:30,439 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:21:31,011 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6702MB, Used=1490MB, Baseline=1096MB, Model=394MB
Tracked model load: moondream-2 - VRAM: 394MB, RAM: 1115MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:21:33,190 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 21:21:33,211 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:21:33,788 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6676MB, Used=1516MB, Baseline=1096MB, Model=420MB
Tracked model load: moondream-2 - VRAM: 420MB, RAM: 1115MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
2026-01-02 21:21:37,278 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2654MB, Used=5538MB, Baseline=1096MB, Model=4442MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4442MB, RAM: 1114MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:21:39,148 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4076.19 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=2605MB, Used=5587MB, Baseline=1096MB, Model=4491MB
Tracked model load: moondream-2 - VRAM: 4491MB, RAM: 1114MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 21:21:43,546 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 4271204352
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:21:43,991 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
VRAM: Total=8192MB, Free=2651MB, Used=5541MB, Baseline=1096MB, Model=4444MB
Tracked model load: moondream-2 - VRAM: 4444MB, RAM: 1114MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4772.28 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=1932MB, Used=6260MB, Baseline=1096MB, Model=5164MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 5164MB, RAM: 1834MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:21:46,234 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=2651MB, Used=5541MB, Baseline=1096MB, Model=4445MB
Tracked model load: moondream-2 - VRAM: 4445MB, RAM: 1115MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 21:21:52,155 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 4271204352
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:21:54,258 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
VRAM: Total=8192MB, Free=2668MB, Used=5524MB, Baseline=1096MB, Model=4427MB
Tracked model load: moondream-2 - VRAM: 4427MB, RAM: 1115MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 21:37:29,996 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:37:40,554 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6723MB, Used=1469MB, Baseline=1096MB, Model=373MB
Tracked model load: moondream-2 - VRAM: 373MB, RAM: 1115MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:37:43,770 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2678MB, Used=5514MB, Baseline=1096MB, Model=4418MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4418MB, RAM: 1115MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:37:46,472 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6735MB, Used=1457MB, Baseline=1096MB, Model=361MB
Tracked model load: moondream-2 - VRAM: 361MB, RAM: 1115MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:37:49,531 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 21:37:50,369 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:37:50,960 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6713MB, Used=1479MB, Baseline=1096MB, Model=383MB
Tracked model load: moondream-2 - VRAM: 383MB, RAM: 1115MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:37:53,994 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2676MB, Used=5516MB, Baseline=1096MB, Model=4419MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4419MB, RAM: 1115MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:37:57,957 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6718MB, Used=1474MB, Baseline=1096MB, Model=377MB
Tracked model load: moondream-2 - VRAM: 377MB, RAM: 1115MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:38:01,013 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 21:38:01,908 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:38:02,480 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6699MB, Used=1493MB, Baseline=1096MB, Model=397MB
Tracked model load: moondream-2 - VRAM: 397MB, RAM: 1115MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:38:05,447 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4076.19 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2668MB, Used=5524MB, Baseline=1096MB, Model=4428MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4428MB, RAM: 1116MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:38:11,189 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4210.42 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=2534MB, Used=5658MB, Baseline=1096MB, Model=4562MB
Tracked model load: moondream-2 - VRAM: 4562MB, RAM: 1116MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=6720MB, Used=1472MB, Baseline=1096MB, Model=376MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 376MB, RAM: 1124MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:38:12,260 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6720MB, Used=1472MB, Baseline=1096MB, Model=376MB
Tracked model load: moondream-2 - VRAM: 376MB, RAM: 1124MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:38:14,222 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 21:38:15,327 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:38:15,975 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6678MB, Used=1514MB, Baseline=1096MB, Model=418MB
Tracked model load: moondream-2 - VRAM: 418MB, RAM: 1235MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
2026-01-02 21:38:17,763 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 21:38:19,145 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:38:19,715 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6685MB, Used=1507MB, Baseline=1096MB, Model=411MB
Tracked model load: moondream-2 - VRAM: 411MB, RAM: 1249MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:38:21,273 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2688MB, Used=5504MB, Baseline=1096MB, Model=4408MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4408MB, RAM: 1241MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:38:30,893 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=2659MB, Used=5533MB, Baseline=1096MB, Model=4436MB
Tracked model load: moondream-2 - VRAM: 4436MB, RAM: 1241MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 21:38:35,042 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 4271204352
[InferenceService] Unloading model...
[InferenceService] Unloading backends from manifest...
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 4271204352
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:38:45,589 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
VRAM: Total=8192MB, Free=2661MB, Used=5531MB, Baseline=1096MB, Model=4435MB
Tracked model load: moondream-2 - VRAM: 4435MB, RAM: 1243MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 21:38:54,286 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:39:04,820 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6767MB, Used=1425MB, Baseline=1096MB, Model=329MB
Tracked model load: moondream-2 - VRAM: 329MB, RAM: 1244MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:39:07,759 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2414MB, Used=5778MB, Baseline=1096MB, Model=4682MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4682MB, RAM: 1244MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:39:11,646 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6468MB, Used=1724MB, Baseline=1096MB, Model=628MB
Tracked model load: moondream-2 - VRAM: 628MB, RAM: 1244MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:39:14,712 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 21:39:15,487 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:39:16,066 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6364MB, Used=1828MB, Baseline=1096MB, Model=732MB
Tracked model load: moondream-2 - VRAM: 732MB, RAM: 1243MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:39:19,046 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2378MB, Used=5814MB, Baseline=1096MB, Model=4718MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4718MB, RAM: 1243MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:39:23,214 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6420MB, Used=1772MB, Baseline=1096MB, Model=676MB
Tracked model load: moondream-2 - VRAM: 676MB, RAM: 1243MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:39:26,755 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 21:39:27,742 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:39:28,414 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6383MB, Used=1809MB, Baseline=1096MB, Model=713MB
Tracked model load: moondream-2 - VRAM: 713MB, RAM: 1243MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:39:31,387 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4076.19 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2355MB, Used=5837MB, Baseline=1096MB, Model=4741MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4741MB, RAM: 1243MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:39:37,455 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4210.45 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=2222MB, Used=5970MB, Baseline=1096MB, Model=4874MB
Tracked model load: moondream-2 - VRAM: 4874MB, RAM: 1243MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=6408MB, Used=1784MB, Baseline=1096MB, Model=688MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 688MB, RAM: 1243MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:39:38,477 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6408MB, Used=1784MB, Baseline=1096MB, Model=688MB
Tracked model load: moondream-2 - VRAM: 688MB, RAM: 1243MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:39:40,427 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 21:39:41,415 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:39:42,124 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6376MB, Used=1816MB, Baseline=1096MB, Model=720MB
Tracked model load: moondream-2 - VRAM: 720MB, RAM: 1287MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:39:44,096 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 21:39:45,045 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:39:45,694 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6407MB, Used=1785MB, Baseline=1096MB, Model=688MB
Tracked model load: moondream-2 - VRAM: 688MB, RAM: 1308MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:39:47,669 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 21:39:55,647 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 4271204352
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:39:57,652 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
VRAM: Total=8192MB, Free=2309MB, Used=5883MB, Baseline=1096MB, Model=4787MB
Tracked model load: moondream-2 - VRAM: 4787MB, RAM: 1309MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 21:39:59,709 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 4271204352
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:40:14,685 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6658MB, Used=1534MB, Baseline=1096MB, Model=438MB
Tracked model load: moondream-2 - VRAM: 438MB, RAM: 1309MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:40:17,756 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2611MB, Used=5581MB, Baseline=1096MB, Model=4485MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4485MB, RAM: 1309MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:40:21,670 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6634MB, Used=1558MB, Baseline=1096MB, Model=462MB
Tracked model load: moondream-2 - VRAM: 462MB, RAM: 1309MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:40:24,717 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 21:40:25,595 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:40:26,161 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6614MB, Used=1578MB, Baseline=1096MB, Model=482MB
Tracked model load: moondream-2 - VRAM: 482MB, RAM: 1309MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:40:29,109 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2600MB, Used=5592MB, Baseline=1096MB, Model=4496MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4496MB, RAM: 1310MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:40:32,382 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6642MB, Used=1550MB, Baseline=1096MB, Model=454MB
Tracked model load: moondream-2 - VRAM: 454MB, RAM: 1310MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:40:35,412 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 21:40:36,187 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:40:36,683 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6638MB, Used=1554MB, Baseline=1096MB, Model=458MB
Tracked model load: moondream-2 - VRAM: 458MB, RAM: 1310MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:40:39,565 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2613MB, Used=5579MB, Baseline=1096MB, Model=4483MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4483MB, RAM: 1310MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:40:44,968 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4076.19 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=2613MB, Used=5579MB, Baseline=1096MB, Model=4483MB
Tracked model load: moondream-2 - VRAM: 4483MB, RAM: 1310MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=6646MB, Used=1546MB, Baseline=1096MB, Model=449MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 449MB, RAM: 1310MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:40:46,608 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6642MB, Used=1550MB, Baseline=1096MB, Model=454MB
Tracked model load: moondream-2 - VRAM: 454MB, RAM: 1310MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:40:48,564 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 21:40:49,627 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:40:50,374 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6623MB, Used=1569MB, Baseline=1096MB, Model=472MB
Tracked model load: moondream-2 - VRAM: 472MB, RAM: 1310MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
2026-01-02 21:40:52,757 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 21:40:53,888 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 41403904
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:40:54,547 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 1056.99 MB
VRAM: Total=8192MB, Free=5579MB, Used=2613MB, Baseline=1096MB, Model=1517MB
Tracked model load: moondream-2 - VRAM: 1517MB, RAM: 2366MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:40:55,781 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2601MB, Used=5591MB, Baseline=1096MB, Model=4494MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4494MB, RAM: 1311MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:41:02,917 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=2617MB, Used=5575MB, Baseline=1096MB, Model=4479MB
Tracked model load: moondream-2 - VRAM: 4479MB, RAM: 1311MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 21:41:06,844 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 4271204352
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:41:22,779 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6706MB, Used=1486MB, Baseline=1096MB, Model=390MB
Tracked model load: moondream-2 - VRAM: 390MB, RAM: 1313MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:41:25,931 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2627MB, Used=5565MB, Baseline=1096MB, Model=4469MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4469MB, RAM: 1313MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:41:28,841 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6689MB, Used=1503MB, Baseline=1096MB, Model=407MB
Tracked model load: moondream-2 - VRAM: 407MB, RAM: 1313MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
2026-01-02 21:41:31,898 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 21:41:32,678 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
2026-01-02 21:41:36,083 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:41:37,077 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6696MB, Used=1496MB, Baseline=1096MB, Model=400MB
Tracked model load: moondream-2 - VRAM: 400MB, RAM: 1313MB
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:41:40,096 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 21:41:40,854 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:41:41,435 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6658MB, Used=1534MB, Baseline=1096MB, Model=438MB
Tracked model load: moondream-2 - VRAM: 438MB, RAM: 1313MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:41:44,381 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4076.19 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2621MB, Used=5571MB, Baseline=1096MB, Model=4475MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4475MB, RAM: 1313MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:41:50,240 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4210.42 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=2501MB, Used=5691MB, Baseline=1096MB, Model=4595MB
Tracked model load: moondream-2 - VRAM: 4595MB, RAM: 1313MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=6708MB, Used=1484MB, Baseline=1096MB, Model=388MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 388MB, RAM: 1313MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:41:51,512 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6675MB, Used=1517MB, Baseline=1096MB, Model=421MB
Tracked model load: moondream-2 - VRAM: 421MB, RAM: 1329MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:41:53,150 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 21:41:54,263 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:41:54,897 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6648MB, Used=1544MB, Baseline=1096MB, Model=448MB
Tracked model load: moondream-2 - VRAM: 448MB, RAM: 1314MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:41:56,866 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 21:41:58,116 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:41:58,796 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6715MB, Used=1477MB, Baseline=1096MB, Model=381MB
Tracked model load: moondream-2 - VRAM: 381MB, RAM: 1329MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:42:00,622 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=6693MB, Used=1499MB, Baseline=1096MB, Model=402MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 402MB, RAM: 1329MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:42:02,957 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 2209.50 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=4537MB, Used=3655MB, Baseline=1096MB, Model=2558MB
Tracked model load: moondream-2 - VRAM: 2558MB, RAM: 3518MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:42:04,098 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 21:42:07,151 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 4271204352
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:42:07,691 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
VRAM: Total=8192MB, Free=2650MB, Used=5542MB, Baseline=1096MB, Model=4446MB
Tracked model load: moondream-2 - VRAM: 4446MB, RAM: 1314MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2654MB, Used=5538MB, Baseline=1096MB, Model=4442MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4442MB, RAM: 1336MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:42:09,556 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 5409.34 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=1310MB, Used=6882MB, Baseline=1096MB, Model=5786MB
Tracked model load: moondream-2 - VRAM: 5786MB, RAM: 2663MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 21:42:15,651 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 4271204352
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:42:17,882 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
VRAM: Total=8192MB, Free=2651MB, Used=5541MB, Baseline=1096MB, Model=4445MB
Tracked model load: moondream-2 - VRAM: 4445MB, RAM: 1315MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 21:42:39,836 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:42:50,560 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6724MB, Used=1468MB, Baseline=1096MB, Model=372MB
Tracked model load: moondream-2 - VRAM: 372MB, RAM: 1315MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:42:53,513 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2682MB, Used=5510MB, Baseline=1096MB, Model=4414MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4414MB, RAM: 1315MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:42:57,159 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6721MB, Used=1471MB, Baseline=1096MB, Model=375MB
Tracked model load: moondream-2 - VRAM: 375MB, RAM: 1315MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:43:00,283 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 21:43:01,239 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:43:01,720 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6693MB, Used=1499MB, Baseline=1096MB, Model=403MB
Tracked model load: moondream-2 - VRAM: 403MB, RAM: 1314MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:43:04,671 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2649MB, Used=5543MB, Baseline=1096MB, Model=4447MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4447MB, RAM: 1314MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:43:07,085 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6695MB, Used=1497MB, Baseline=1096MB, Model=401MB
Tracked model load: moondream-2 - VRAM: 401MB, RAM: 1314MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:43:10,097 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 21:43:10,731 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:43:11,225 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6671MB, Used=1521MB, Baseline=1096MB, Model=424MB
Tracked model load: moondream-2 - VRAM: 424MB, RAM: 1314MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:43:14,956 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2692MB, Used=5500MB, Baseline=1096MB, Model=4404MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4404MB, RAM: 1314MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:43:21,677 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4076.19 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=2671MB, Used=5521MB, Baseline=1096MB, Model=4424MB
Tracked model load: moondream-2 - VRAM: 4424MB, RAM: 1314MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=6741MB, Used=1451MB, Baseline=1096MB, Model=355MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 355MB, RAM: 1375MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:43:23,637 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6719MB, Used=1473MB, Baseline=1096MB, Model=376MB
Tracked model load: moondream-2 - VRAM: 376MB, RAM: 1375MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:43:25,108 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 21:43:26,388 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:43:27,040 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6693MB, Used=1499MB, Baseline=1096MB, Model=403MB
Tracked model load: moondream-2 - VRAM: 403MB, RAM: 1422MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
2026-01-02 21:43:29,430 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 21:43:30,672 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 49374208
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 21:43:31,450 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 1441.16 MB
VRAM: Total=8192MB, Free=5239MB, Used=2953MB, Baseline=1096MB, Model=1857MB
Tracked model load: moondream-2 - VRAM: 1857MB, RAM: 2879MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 21:43:32,780 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 21:43:59,311 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:00:51,620 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6222MB, Used=1970MB, Baseline=1096MB, Model=874MB
Tracked model load: moondream-2 - VRAM: 874MB, RAM: 1440MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
2026-01-02 22:00:54,631 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2273MB, Used=5919MB, Baseline=1096MB, Model=4823MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4823MB, RAM: 1441MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:00:58,020 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6330MB, Used=1862MB, Baseline=1096MB, Model=766MB
Tracked model load: moondream-2 - VRAM: 766MB, RAM: 1441MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
2026-01-02 22:01:01,089 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 22:01:01,984 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:01:02,511 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6361MB, Used=1831MB, Baseline=1096MB, Model=735MB
Tracked model load: moondream-2 - VRAM: 735MB, RAM: 1441MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
2026-01-02 22:01:05,451 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2312MB, Used=5880MB, Baseline=1096MB, Model=4783MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4783MB, RAM: 1441MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:01:09,046 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6349MB, Used=1843MB, Baseline=1096MB, Model=746MB
Tracked model load: moondream-2 - VRAM: 746MB, RAM: 1441MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
2026-01-02 22:01:12,118 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 22:01:12,895 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:01:13,412 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6321MB, Used=1871MB, Baseline=1096MB, Model=775MB
Tracked model load: moondream-2 - VRAM: 775MB, RAM: 1441MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
2026-01-02 22:01:16,931 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4076.19 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2302MB, Used=5890MB, Baseline=1096MB, Model=4793MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4793MB, RAM: 1441MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:01:22,678 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4210.42 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=2160MB, Used=6032MB, Baseline=1096MB, Model=4936MB
Tracked model load: moondream-2 - VRAM: 4936MB, RAM: 1441MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=6343MB, Used=1849MB, Baseline=1096MB, Model=753MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 753MB, RAM: 1441MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:01:23,829 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6343MB, Used=1849MB, Baseline=1096MB, Model=753MB
Tracked model load: moondream-2 - VRAM: 753MB, RAM: 1441MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
2026-01-02 22:01:25,859 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.[InferenceService] Unloading model...

[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 22:01:28,070 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:01:28,699 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6411MB, Used=1781MB, Baseline=1096MB, Model=685MB
Tracked model load: moondream-2 - VRAM: 685MB, RAM: 1442MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
2026-01-02 22:01:31,200 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 22:01:32,425 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 417806336
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:01:33,081 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 1505.19 MB
VRAM: Total=8192MB, Free=5120MB, Used=3072MB, Baseline=1096MB, Model=1976MB
Tracked model load: moondream-2 - VRAM: 1976MB, RAM: 2915MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 22:01:34,355 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2589MB, Used=5603MB, Baseline=1096MB, Model=4507MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4507MB, RAM: 1442MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:01:41,584 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=2607MB, Used=5585MB, Baseline=1096MB, Model=4489MB
Tracked model load: moondream-2 - VRAM: 4489MB, RAM: 1442MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 22:01:45,561 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 4271204352
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:02:09,191 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6626MB, Used=1566MB, Baseline=1096MB, Model=470MB
Tracked model load: moondream-2 - VRAM: 470MB, RAM: 1444MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
2026-01-02 22:02:12,298 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2606MB, Used=5586MB, Baseline=1096MB, Model=4489MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4489MB, RAM: 1443MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:02:13,614 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6671MB, Used=1521MB, Baseline=1096MB, Model=425MB
Tracked model load: moondream-2 - VRAM: 425MB, RAM: 1443MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
2026-01-02 22:02:16,673 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 22:02:17,466 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:08:50,343 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6480MB, Used=1712MB, Baseline=1096MB, Model=616MB
Tracked model load: moondream-2 - VRAM: 616MB, RAM: 1443MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
2026-01-02 22:08:53,344 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2427MB, Used=5765MB, Baseline=1096MB, Model=4669MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4669MB, RAM: 1443MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:08:56,345 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6493MB, Used=1699MB, Baseline=1096MB, Model=603MB
Tracked model load: moondream-2 - VRAM: 603MB, RAM: 1443MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
2026-01-02 22:08:59,937 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 22:09:00,760 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:09:01,328 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6483MB, Used=1709MB, Baseline=1096MB, Model=613MB
Tracked model load: moondream-2 - VRAM: 613MB, RAM: 1444MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
2026-01-02 22:09:04,358 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2466MB, Used=5726MB, Baseline=1096MB, Model=4630MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4630MB, RAM: 1444MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:09:08,936 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6495MB, Used=1697MB, Baseline=1096MB, Model=600MB
Tracked model load: moondream-2 - VRAM: 600MB, RAM: 1444MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
2026-01-02 22:09:11,988 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 22:09:12,810 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:09:13,267 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6480MB, Used=1712MB, Baseline=1096MB, Model=616MB
Tracked model load: moondream-2 - VRAM: 616MB, RAM: 1444MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
2026-01-02 22:09:16,636 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2464MB, Used=5728MB, Baseline=1096MB, Model=4632MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4632MB, RAM: 1444MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:09:21,564 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4076.19 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=2447MB, Used=5745MB, Baseline=1096MB, Model=4649MB
Tracked model load: moondream-2 - VRAM: 4649MB, RAM: 1444MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=6502MB, Used=1690MB, Baseline=1096MB, Model=594MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 594MB, RAM: 1459MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:09:23,279 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6498MB, Used=1694MB, Baseline=1096MB, Model=598MB
Tracked model load: moondream-2 - VRAM: 598MB, RAM: 1461MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 22:09:24,658 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 22:09:25,765 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:09:26,297 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6508MB, Used=1684MB, Baseline=1096MB, Model=588MB
Tracked model load: moondream-2 - VRAM: 588MB, RAM: 1490MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 22:09:28,465 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 22:09:29,321 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:09:29,850 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6500MB, Used=1692MB, Baseline=1096MB, Model=596MB
Tracked model load: moondream-2 - VRAM: 596MB, RAM: 1521MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
2026-01-02 22:09:31,336 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2491MB, Used=5701MB, Baseline=1096MB, Model=4605MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4605MB, RAM: 1506MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:09:38,173 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=2491MB, Used=5701MB, Baseline=1096MB, Model=4605MB
Tracked model load: moondream-2 - VRAM: 4605MB, RAM: 1506MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 22:09:42,002 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 4271204352
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:09:58,435 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6560MB, Used=1632MB, Baseline=1096MB, Model=536MB
Tracked model load: moondream-2 - VRAM: 536MB, RAM: 1506MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
2026-01-02 22:10:01,546 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2501MB, Used=5691MB, Baseline=1096MB, Model=4595MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4595MB, RAM: 1507MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:10:05,428 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6547MB, Used=1645MB, Baseline=1096MB, Model=549MB
Tracked model load: moondream-2 - VRAM: 549MB, RAM: 1507MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
2026-01-02 22:10:08,451 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 22:10:09,328 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
2026-01-02 22:10:12,354 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:10:13,531 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6537MB, Used=1655MB, Baseline=1096MB, Model=559MB
Tracked model load: moondream-2 - VRAM: 559MB, RAM: 1506MB
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
2026-01-02 22:10:16,595 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 22:10:17,454 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:10:18,023 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6508MB, Used=1684MB, Baseline=1096MB, Model=588MB
Tracked model load: moondream-2 - VRAM: 588MB, RAM: 1507MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
2026-01-02 22:10:20,976 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4076.19 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2283MB, Used=5909MB, Baseline=1096MB, Model=4813MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4813MB, RAM: 1507MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:10:25,915 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4210.44 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=2174MB, Used=6018MB, Baseline=1096MB, Model=4922MB
Tracked model load: moondream-2 - VRAM: 4922MB, RAM: 1507MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=6368MB, Used=1824MB, Baseline=1096MB, Model=728MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 728MB, RAM: 1523MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:10:27,742 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6370MB, Used=1822MB, Baseline=1096MB, Model=726MB
Tracked model load: moondream-2 - VRAM: 726MB, RAM: 1523MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
2026-01-02 22:10:29,353 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 22:10:30,377 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:10:30,919 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6444MB, Used=1748MB, Baseline=1096MB, Model=651MB
Tracked model load: moondream-2 - VRAM: 651MB, RAM: 1507MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 22:10:33,285 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 22:10:34,242 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 49374208
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:10:34,842 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 601.93 MB
VRAM: Total=8192MB, Free=5868MB, Used=2324MB, Baseline=1096MB, Model=1228MB
Tracked model load: moondream-2 - VRAM: 1228MB, RAM: 2109MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
2026-01-02 22:10:36,165 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 22:10:47,708 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
2026-01-02 22:10:50,802 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:10:51,798 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
VRAM: Total=8192MB, Free=2517MB, Used=5675MB, Baseline=1096MB, Model=4579MB
Tracked model load: moondream-2 - VRAM: 4579MB, RAM: 1508MB
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 22:10:55,784 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 4271204352
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:11:13,205 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6533MB, Used=1659MB, Baseline=1096MB, Model=563MB
Tracked model load: moondream-2 - VRAM: 563MB, RAM: 1508MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
2026-01-02 22:11:16,374 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2341MB, Used=5851MB, Baseline=1096MB, Model=4755MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4755MB, RAM: 1508MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:11:19,625 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6399MB, Used=1793MB, Baseline=1096MB, Model=696MB
Tracked model load: moondream-2 - VRAM: 696MB, RAM: 1508MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
2026-01-02 22:11:22,691 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 22:11:23,448 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:11:23,903 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6381MB, Used=1811MB, Baseline=1096MB, Model=715MB
Tracked model load: moondream-2 - VRAM: 715MB, RAM: 1508MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
2026-01-02 22:11:27,252 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2296MB, Used=5896MB, Baseline=1096MB, Model=4800MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4800MB, RAM: 1508MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:11:29,922 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6340MB, Used=1852MB, Baseline=1096MB, Model=756MB
Tracked model load: moondream-2 - VRAM: 756MB, RAM: 1508MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
2026-01-02 22:11:33,393 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 22:11:34,220 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:11:34,768 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6416MB, Used=1776MB, Baseline=1096MB, Model=680MB
Tracked model load: moondream-2 - VRAM: 680MB, RAM: 1508MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
2026-01-02 22:11:37,719 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4076.19 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2418MB, Used=5774MB, Baseline=1096MB, Model=4678MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4678MB, RAM: 1508MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:11:43,865 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4210.46 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=2312MB, Used=5880MB, Baseline=1096MB, Model=4784MB
Tracked model load: moondream-2 - VRAM: 4784MB, RAM: 1508MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=6469MB, Used=1723MB, Baseline=1096MB, Model=627MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 627MB, RAM: 1508MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:11:44,964 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6465MB, Used=1727MB, Baseline=1096MB, Model=631MB
Tracked model load: moondream-2 - VRAM: 631MB, RAM: 1508MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
2026-01-02 22:11:46,917 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 22:11:47,979 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:11:48,508 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6438MB, Used=1754MB, Baseline=1096MB, Model=658MB
Tracked model load: moondream-2 - VRAM: 658MB, RAM: 1508MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 22:11:50,635 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 22:11:51,513 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:11:52,117 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6424MB, Used=1768MB, Baseline=1096MB, Model=672MB
Tracked model load: moondream-2 - VRAM: 672MB, RAM: 1524MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
2026-01-02 22:11:53,927 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 22:12:02,854 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 4271204352
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:12:05,332 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
VRAM: Total=8192MB, Free=2296MB, Used=5896MB, Baseline=1096MB, Model=4800MB
Tracked model load: moondream-2 - VRAM: 4800MB, RAM: 1509MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 22:12:07,463 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 4271204352
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:12:20,601 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6438MB, Used=1754MB, Baseline=1096MB, Model=658MB
Tracked model load: moondream-2 - VRAM: 658MB, RAM: 1509MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
2026-01-02 22:12:23,688 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2442MB, Used=5750MB, Baseline=1096MB, Model=4654MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4654MB, RAM: 1509MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:12:27,162 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6484MB, Used=1708MB, Baseline=1096MB, Model=612MB
Tracked model load: moondream-2 - VRAM: 612MB, RAM: 1509MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
2026-01-02 22:12:30,245 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 22:12:31,231 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:12:31,779 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6470MB, Used=1722MB, Baseline=1096MB, Model=626MB
Tracked model load: moondream-2 - VRAM: 626MB, RAM: 1509MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
2026-01-02 22:12:34,708 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2463MB, Used=5729MB, Baseline=1096MB, Model=4633MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4633MB, RAM: 1509MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:12:38,767 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6497MB, Used=1695MB, Baseline=1096MB, Model=598MB
Tracked model load: moondream-2 - VRAM: 598MB, RAM: 1509MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
2026-01-02 22:12:41,996 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 22:12:42,944 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:12:43,492 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6422MB, Used=1770MB, Baseline=1096MB, Model=674MB
Tracked model load: moondream-2 - VRAM: 674MB, RAM: 1509MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
2026-01-02 22:12:46,852 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4076.19 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2446MB, Used=5746MB, Baseline=1096MB, Model=4650MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4650MB, RAM: 1509MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:12:52,838 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4210.44 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=2350MB, Used=5842MB, Baseline=1096MB, Model=4746MB
Tracked model load: moondream-2 - VRAM: 4746MB, RAM: 1509MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=6516MB, Used=1676MB, Baseline=1096MB, Model=580MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 580MB, RAM: 1509MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:12:54,213 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6518MB, Used=1674MB, Baseline=1096MB, Model=578MB
Tracked model load: moondream-2 - VRAM: 578MB, RAM: 1524MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
2026-01-02 22:12:55,966 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 22:12:57,400 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:12:58,079 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6437MB, Used=1755MB, Baseline=1096MB, Model=659MB
Tracked model load: moondream-2 - VRAM: 659MB, RAM: 1524MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
2026-01-02 22:13:00,361 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 22:13:01,406 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 49374208
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:13:01,997 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 224.04 MB
VRAM: Total=8192MB, Free=6276MB, Used=1916MB, Baseline=1096MB, Model=820MB
Tracked model load: moondream-2 - VRAM: 820MB, RAM: 1741MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
2026-01-02 22:13:03,363 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 22:13:14,557 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 4271204352
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
[InferenceService] Unloading model...
[InferenceService] Unloading backends from manifest...
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 4271204352
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:13:30,367 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6598MB, Used=1594MB, Baseline=1096MB, Model=498MB
Tracked model load: moondream-2 - VRAM: 498MB, RAM: 1510MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
2026-01-02 22:13:33,681 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2546MB, Used=5646MB, Baseline=1096MB, Model=4550MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4550MB, RAM: 1568MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:13:37,289 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6580MB, Used=1612MB, Baseline=1096MB, Model=516MB
Tracked model load: moondream-2 - VRAM: 516MB, RAM: 1568MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
2026-01-02 22:13:40,338 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 22:13:41,373 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:13:41,952 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6587MB, Used=1605MB, Baseline=1096MB, Model=508MB
Tracked model load: moondream-2 - VRAM: 508MB, RAM: 1573MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
2026-01-02 22:13:44,885 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2579MB, Used=5613MB, Baseline=1096MB, Model=4517MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4517MB, RAM: 1573MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:13:47,541 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6621MB, Used=1571MB, Baseline=1096MB, Model=475MB
Tracked model load: moondream-2 - VRAM: 475MB, RAM: 1573MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
2026-01-02 22:13:50,596 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 22:13:51,217 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:13:51,761 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6582MB, Used=1610MB, Baseline=1096MB, Model=513MB
Tracked model load: moondream-2 - VRAM: 513MB, RAM: 1572MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
2026-01-02 22:13:54,734 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2573MB, Used=5619MB, Baseline=1096MB, Model=4523MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4523MB, RAM: 1572MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:14:00,212 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4076.19 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=2553MB, Used=5639MB, Baseline=1096MB, Model=4543MB
Tracked model load: moondream-2 - VRAM: 4543MB, RAM: 1572MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=6619MB, Used=1573MB, Baseline=1096MB, Model=477MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 477MB, RAM: 1572MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:14:01,690 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6619MB, Used=1573MB, Baseline=1096MB, Model=476MB
Tracked model load: moondream-2 - VRAM: 476MB, RAM: 1588MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
2026-01-02 22:14:03,142 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 22:14:04,362 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:14:04,931 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6586MB, Used=1606MB, Baseline=1096MB, Model=510MB
Tracked model load: moondream-2 - VRAM: 510MB, RAM: 1573MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
2026-01-02 22:14:07,678 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 22:14:08,441 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:14:09,150 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6573MB, Used=1619MB, Baseline=1096MB, Model=522MB
Tracked model load: moondream-2 - VRAM: 522MB, RAM: 1589MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
2026-01-02 22:14:10,693 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2620MB, Used=5572MB, Baseline=1096MB, Model=4476MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4476MB, RAM: 1573MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:14:18,609 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=2619MB, Used=5573MB, Baseline=1096MB, Model=4477MB
Tracked model load: moondream-2 - VRAM: 4477MB, RAM: 1573MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 22:14:22,796 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 4271204352
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:14:37,600 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6369MB, Used=1823MB, Baseline=1096MB, Model=727MB
Tracked model load: moondream-2 - VRAM: 727MB, RAM: 1574MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
2026-01-02 22:14:41,213 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2385MB, Used=5807MB, Baseline=1096MB, Model=4711MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4711MB, RAM: 1575MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:14:44,287 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6441MB, Used=1751MB, Baseline=1096MB, Model=655MB
Tracked model load: moondream-2 - VRAM: 655MB, RAM: 1575MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
2026-01-02 22:14:47,365 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 22:14:48,162 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
2026-01-02 22:14:51,224 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:14:52,378 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6430MB, Used=1762MB, Baseline=1096MB, Model=666MB
Tracked model load: moondream-2 - VRAM: 666MB, RAM: 1574MB
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
2026-01-02 22:14:55,462 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 22:14:56,306 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:14:56,898 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6355MB, Used=1837MB, Baseline=1096MB, Model=741MB
Tracked model load: moondream-2 - VRAM: 741MB, RAM: 1574MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
2026-01-02 22:14:59,913 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4076.19 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2386MB, Used=5806MB, Baseline=1096MB, Model=4710MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4710MB, RAM: 1574MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:15:06,839 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4210.42 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=2264MB, Used=5928MB, Baseline=1096MB, Model=4832MB
Tracked model load: moondream-2 - VRAM: 4832MB, RAM: 1574MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=6412MB, Used=1780MB, Baseline=1096MB, Model=684MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 684MB, RAM: 1574MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:15:08,209 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6420MB, Used=1772MB, Baseline=1096MB, Model=676MB
Tracked model load: moondream-2 - VRAM: 676MB, RAM: 1590MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
2026-01-02 22:15:09,976 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 22:15:11,391 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:15:12,058 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6352MB, Used=1840MB, Baseline=1096MB, Model=744MB
Tracked model load: moondream-2 - VRAM: 744MB, RAM: 1590MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
2026-01-02 22:15:13,809 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 22:15:14,442 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:15:15,083 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6412MB, Used=1780MB, Baseline=1096MB, Model=684MB
Tracked model load: moondream-2 - VRAM: 684MB, RAM: 1575MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
2026-01-02 22:15:18,125 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4076.19 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2349MB, Used=5843MB, Baseline=1096MB, Model=4747MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4747MB, RAM: 1575MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:15:20,258 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4210.44 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=2208MB, Used=5984MB, Baseline=1096MB, Model=4888MB
Tracked model load: moondream-2 - VRAM: 4888MB, RAM: 1575MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2561MB, Used=5631MB, Baseline=1096MB, Model=4535MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4535MB, RAM: 1575MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:15:24,751 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=2561MB, Used=5631MB, Baseline=1096MB, Model=4535MB
Tracked model load: moondream-2 - VRAM: 4535MB, RAM: 1575MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 22:15:26,842 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 4271204352
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:15:27,361 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
VRAM: Total=8192MB, Free=2388MB, Used=5804MB, Baseline=1096MB, Model=4708MB
Tracked model load: moondream-2 - VRAM: 4708MB, RAM: 2148MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 22:15:28,934 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 4311010304
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:15:29,462 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 5601.43 MB
VRAM: Total=8192MB, Free=889MB, Used=7303MB, Baseline=1096MB, Model=6207MB
Tracked model load: moondream-2 - VRAM: 6207MB, RAM: 3122MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 22:15:47,297 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:15:57,555 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6528MB, Used=1664MB, Baseline=1096MB, Model=568MB
Tracked model load: moondream-2 - VRAM: 568MB, RAM: 1576MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
2026-01-02 22:16:00,525 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2507MB, Used=5685MB, Baseline=1096MB, Model=4589MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4589MB, RAM: 1576MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:16:02,360 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6552MB, Used=1640MB, Baseline=1096MB, Model=544MB
Tracked model load: moondream-2 - VRAM: 544MB, RAM: 1576MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
2026-01-02 22:16:05,461 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 22:16:06,362 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Service stopped. Auto-starting moondream-2...
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:16:06,973 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
VRAM: Total=8192MB, Free=6466MB, Used=1726MB, Baseline=1096MB, Model=630MB
Tracked model load: moondream-2 - VRAM: 630MB, RAM: 1576MB
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
2026-01-02 22:16:09,969 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Smart Switching (Mode: low) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
Auto-switching to requested model: analysis/wd-vit-tagger-v3
[Manifest] Auto-mapping dynamic model 'analysis/wd-vit-tagger-v3' to 'wd14_backend'
DEBUG: download_backend called for wd14_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py
DEBUG: Backend file exists.
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 4073.34 MB
Tracked model unload: moondream-2
[Tracker] Unloaded previous model on auto-switch: moondream-2
VRAM: Total=8192MB, Free=2463MB, Used=5729MB, Baseline=1096MB, Model=4633MB
Tracked model load: analysis/wd-vit-tagger-v3 - VRAM: 4633MB, RAM: 1576MB
DEBUG: execute_function called for 'classify'
DEBUG: backend object: <module 'wd14_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/wd14_backend/backend.py'>
DEBUG: backend dir: ['AutoImageProcessor', 'AutoModelForImageClassification', 'Backend', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_backend', 'batch-caption', 'batch_caption', 'caption', 'np', 'os', 'query', 'sys', 'torch']
DEBUG: hasattr failed for 'classify'
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/applications.py", line 1139, in __call__
    await super().__call__(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 120, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/bcoster/.local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 106, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 430, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.local/lib/python3.12/site-packages/fastapi/routing.py", line 316, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 1891, in dynamic_route
    return await self._handle_dynamic_request(request, path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bcoster/.moondream-station/moondream-station/moondream_station/core/rest_server.py", line 2212, in _handle_dynamic_request
    vram_mode == "low"
    ^^^^^^^^^
NameError: name 'vram_mode' is not defined
Auto-switching to requested model: moondream-2
DEBUG: download_backend called for moondream_backend
DEBUG: Checking backend file at /home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py
DEBUG: Backend file exists.
2026-01-02 22:16:14,208 - moondream_backend_worker_0 - INFO - Backend initialized with args: {'model_id': 'vikhyatk/moondream2'}
[InferenceService] Post-Load Cleanup Complete. Active VRAM: 9.12 MB
Tracked model unload: analysis/wd-vit-tagger-v3
[Tracker] Auto-switch unloaded: analysis/wd-vit-tagger-v3
VRAM: Total=8192MB, Free=6509MB, Used=1683MB, Baseline=1096MB, Model=587MB
Tracked model load: moondream-2 - VRAM: 587MB, RAM: 1576MB
[Tracker] Auto-switch loaded: moondream-2
DEBUG: Smart Switching (Mode: balanced) - Unloading SDXL before analysis...
DEBUG: execute_function called for 'query'
DEBUG: backend object: <module 'moondream_backend_worker_0' from '/home/bcoster/.moondream-station/models/backends/moondream_backend/backend.py'>
DEBUG: backend dir: ['AutoModelForCausalLM', 'AutoTokenizer', 'Image', 'ModelService', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_base64_image', '_model_args', '_model_service', 'base64', 'caption', 'detect', 'get_model_service', 'init_backend', 'io', 'logger', 'logging', 'os', 'point', 'query', 'torch', 'unload']
Found local weights for MD2 at /home/bcoster/.moondream-station/models/backends/moondream_backend/weights
Failed to load local weights: [Errno 2] No such file or directory: '/home/bcoster/.cache/huggingface/modules/transformers_modules/weights/fourier_features.py'. Falling back to HF.
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
2026-01-02 22:16:17,314 - moondream_backend_worker_0 - INFO - Model commit hash: 6b714b26eea5cbd9f31e4edb2541c170afa935ba
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
[InferenceService] Unloading model...
[InferenceService] Shutting down worker pool...
[InferenceService] Unloading backends from manifest...
2026-01-02 22:16:18,359 - moondream_backend_worker_0 - INFO - Unloading Moondream backend...
DEBUG: Unloaded moondream_backend_worker_0
DEBUG: torch.cuda.empty_cache() called
[InferenceService] Resetting torch._dynamo...
[InferenceService] Unload complete. CUDA Mem: 9568256
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
DEBUG: Discovering models in /home/bcoster/.moondream-station/models
start_server.sh: line 7: 2917688 Killed                  python3 $DIR/start_server.py
